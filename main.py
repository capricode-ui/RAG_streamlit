# -*- coding: utf-8 -*-
"""Freezed code of Final PineconeQdrantFAISSandChroma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0xv2v3tc6MFMlQ2sxv1cZkGpI3gxEJ2

### Install dependencies
"""


# Call the function


#!pip install "pinecone[grpc]"

"""### Preprocess Text"""

def preprocess_text(word_file_path, size):

  import os


  # Step 2: Read the Word file if it exists
  from docx import Document

  # Path to the Word file in the Kaggle dataset


  # Read the Word file
  doc = Document(word_file_path)

  # Extract text from the document
  full_text = []
  for para in doc.paragraphs:
      full_text.append(para.text)

  # Join all paragraphs into a single string
  document_text = '\n'.join(full_text)
  #print(document_text)
  from langchain.text_splitter import RecursiveCharacterTextSplitter
  from langchain.docstore.document import Document

  text_splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=200)
  # Convert the string to a Document object
  docs = [Document(page_content=document_text)]
  text = text_splitter.split_documents(docs) # Pass the Document object to split_documents
  #/kaggle/input/dataset1-fd/FD.docx
  return text

"""###Preprocess functions for : chroma, faiss, qdrant and pinecone

"""

def preprocess_chroma(text, embedding_model_name, persist_directory):
    from langchain.embeddings import SentenceTransformerEmbeddings
    from langchain.vectorstores import Chroma

    embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)
    vectordb = Chroma.from_documents(documents=text, embedding=embedding_model, persist_directory=persist_directory)
    vectordb.persist()

    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)
    retriever = vectordb.as_retriever()

    return vectordb, retriever

def preprocess_faiss(text, embedding_model_name):
    from langchain.embeddings import SentenceTransformerEmbeddings
    import numpy as np
    import faiss
    from langchain.docstore.in_memory import InMemoryDocstore
    from langchain.vectorstores import FAISS
    from langchain.docstore.document import Document

    embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)
    texts = [doc.page_content for doc in text]
    embeddings = embedding_model.embed_documents(texts)
    embeddings = np.array(embeddings)
    dimension = embeddings.shape[1]

    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    docstore = InMemoryDocstore({i: Document(page_content=texts[i]) for i in range(len(texts))})
    index_to_docstore_id = {i: i for i in range(len(texts))}

    vector_store = FAISS(
        index=index,
        docstore=docstore,
        index_to_docstore_id=index_to_docstore_id,
        embedding_function=embedding_model.embed_query
    )

    return index, docstore, index_to_docstore_id, vector_store

def preprocess_qdrant(text, embeddings, client_url, client_api_key, collection_name, batch_size=250):
    from qdrant_client import QdrantClient, models

    client = QdrantClient(url=client_url, api_key=client_api_key)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(size=embeddings.shape[1], distance=models.Distance.COSINE)
    )

    qdrant_index = list(range(1, len(text) + 1))
    for i in range(0, len(text), batch_size):
        low_idx = min(i + batch_size, len(text))
        batch_of_ids = qdrant_index[i: low_idx]
        batch_of_embs = embeddings[i: low_idx]
        batch_of_payloads = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in text[i: low_idx]]

        client.upsert(
            collection_name=collection_name,
            points=models.Batch(
                ids=batch_of_ids,
                vectors=batch_of_embs.tolist(),
                payloads=batch_of_payloads
            )
        )

    return client

def preprocess_pinecone(text,embedding_model_name):
    import numpy as np
    from langchain.embeddings import SentenceTransformerEmbeddings
    embedding_model= SentenceTransformerEmbeddings(model_name=embedding_model_name)
    # Extract the 'page_content' (text) from each Document object
    texts = [doc.page_content for doc in text]
    embeddings = embedding_model.embed_documents(texts)  # Pass the list of texts
    embeddings = np.array(embeddings)
    embeddings = embeddings.tolist()


    import pinecone
    from pinecone.grpc import PineconeGRPC as Pinecone
    from pinecone import ServerlessSpec
    import uuid

    # ... (your existing code) ...

    index_name = "test4"

    pinecone = Pinecone(
        api_key="pcsk_42Yw14_EaKdaMLiAJfWub3s2sEJYPW3jyXXjdCYkH8Mh8rD8wWJ3pS6oCCC9PGqBNuDTuf",
        environment="us-east-1"
    )
    # Check if the index exists
    indexes = pinecone.list_indexes().names()

    if index_name in indexes:
        pinecone.delete_index(index_name)

    pinecone.create_index(
      name=index_name,
      dimension=len(embeddings[0]),
      metric="cosine",
      spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
      ),
      deletion_protection="disabled"
    )

    pinecone_index = pinecone.Index(index_name)

    upsert_data = []
    for i in range(len(texts)):
      upsert_data.append((str(uuid.uuid4()), embeddings[i], {"text": texts[i]}))

    # Upsert data in batches (adjust batch_size as needed)
    batch_size = 100  # Example batch size
    for i in range(0, len(upsert_data), batch_size):
        batch = upsert_data[i : i + batch_size]
        pinecone_index.upsert(vectors=batch)
    return pinecone_index
    # ... (rest of your upsert code) ...

#!pip install weaviate-client

import numpy as np
import os
from langchain.embeddings import SentenceTransformerEmbeddings
import weaviate
from weaviate.auth import AuthApiKey
import weaviate.classes.config as wvcc
from weaviate.collections import Collection

def preprocess_weaviate(text, embedding_model_name):
    from langchain.embeddings import SentenceTransformerEmbeddings
    import numpy as np
    embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)
    import os
    os.environ["WEAVIATE_URL"] = "https://nzppa9tmraq1upywscvxa.c0.asia-southeast1.gcp.weaviate.cloud"
    os.environ["WEAVIATE_API_KEY"] = "vNJwhXh8lWmptEi5yKxdzywtXcA7WzhPcihO"

    weaviate_url = os.environ["WEAVIATE_URL"]
    weaviate_api_key = os.environ["WEAVIATE_API_KEY"]
    import weaviate
    from weaviate.auth import AuthApiKey

    client = weaviate.connect_to_weaviate_cloud(
        cluster_url=weaviate_url,
        auth_credentials=AuthApiKey(weaviate_api_key),
    )
    from langchain_weaviate.vectorstores import WeaviateVectorStore # Import WeaviateVectorStore

    vs = WeaviateVectorStore.from_documents(
        documents=text,
        embedding=embedding_model,
        client=client
    )

    return vs

"""## Preprocess VectorDB function :
### calls the preprocess function for each vectordb
"""

# ipython-input-12-291a5c8eb7ff
from langchain.embeddings import SentenceTransformerEmbeddings

# Declare embedding_model_global as a global variable
embedding_model_global = None

def preprocess_vectordbs(word_file_path, embedding_model_name, size):
    global embedding_model_global  # Declare embedding_model_global as global within the function

    text = preprocess_text(word_file_path, size)

    persist_directory = 'db'
    # Assign the model directly, not the model name
    embedding_model_global = SentenceTransformerEmbeddings(model_name=embedding_model_name)

    # Process Chroma
    vectordb, retriever = preprocess_chroma(text, embedding_model_name, persist_directory) #embedding_model_name changed to embedding_model_global

    # Process FAISS
    index, docstore, index_to_docstore_id, vector_store = preprocess_faiss(text, embedding_model_name) #embedding_model_name changed to embedding_model_global

    # Process Qdrant
    embeddings = vector_store.index.reconstruct_n(0, len(text))
    client_url = "https://186e02e2-6d10-4b48-baf1-273a91f6c628.us-east4-0.gcp.cloud.qdrant.io:6333"
    client_api_key = "khkhQd22_WZRUBXQg_kL_I08CH3L5HmuHGrbETbVaZlyzCQfyjG0_w"
    collection_name = "text_vectors"
    client = preprocess_qdrant(text, embeddings, client_url, client_api_key, collection_name)
    preprocess_pinecone(text,embedding_model_name)

    #pinecone
    pinecone_index = preprocess_pinecone(text,embedding_model_name)

    #process weaviate
    vs = preprocess_weaviate(text, embedding_model_name)

    return index, docstore, index_to_docstore_id, vector_store, retriever, client,pinecone_index,vs

"""## Inference functions for : chroma, faiss, qdrant and pinecone"""

def inference_chroma(chat_model, question):
    from langchain_together import ChatTogether
    from langchain.prompts import PromptTemplate
    from langchain.chains import RetrievalQA

    chat_model = ChatTogether(
        together_api_key="c51c9bcaa6bf7fae3ce684206311564828c13fa2e91553f915fee01d517ccee9",
        model=chat_model,
    )

    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "You are an expert financial advisor. Use the context to answer questions accurately and concisely.\n"
            "Context: {context}\n\n"
            "Question: {question}\n\n"
            "Answer (be specific and avoid hallucinations):"
        )
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=chat_model,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt_template},
    )

    llm_response = qa_chain(question)
    print(llm_response['result'])

def inference_faiss(chat_model, question):
    from langchain.chains import LLMChain
    from langchain_together import ChatTogether
    from langchain.prompts import PromptTemplate
    import numpy as np

    chat_model = ChatTogether(
        together_api_key="c51c9bcaa6bf7fae3ce684206311564828c13fa2e91553f915fee01d517ccee9",
        model=chat_model,
    )

    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "You are an expert financial advisor. Use the context to answer questions accurately and concisely.\n"
            "Context: {context}\n\n"
            "Question: {question}\n\n"
            "Answer (be specific and avoid hallucinations):"
        )
    )

    qa_chain = LLMChain(
        llm=chat_model,
        prompt=prompt_template
    )

    query_embedding = embedding_model_global.embed_query(question)
    D, I = index.search(np.array([query_embedding]), k=1)

    doc_id = I[0][0]
    document = docstore.search(doc_id)
    context = document.page_content

    answer = qa_chain.run(context=context, question=question, clean_up_tokenization_spaces=False)
    print(answer)

def inference_qdrant(chat_model, question):
    from qdrant_client.http.models import SearchRequest
    from langchain_together import ChatTogether
    import numpy as np

    query_embedding = embedding_model_global.embed_query(question)
    query_embedding = np.array(query_embedding)

    search_results = client.search(
        collection_name="text_vectors",
        query_vector=query_embedding,
        limit=2
    )

    contexts = [result.payload['page_content'] for result in search_results]
    context = "\n".join(contexts)

    prompt = f"""
    You are a helpful assistant. Use the following retrieved documents to answer the question:
    Context:
    {context}
    Question: {question}
    Answer:
    """

    llm = ChatTogether(api_key="c51c9bcaa6bf7fae3ce684206311564828c13fa2e91553f915fee01d517ccee9",
                model=chat_model)

    response = llm.predict(prompt)
    print(response)

def inference_pinecone(chat_model, question):
  import pinecone
  from pinecone import Pinecone
  from langchain_together import ChatTogether
  import numpy as np

  # Initialize Pinecone



  # Step 1: Generate query embedding
  query_embedding = embedding_model_global.embed_query(question)
  query_embedding = np.array(query_embedding)

  # Step 2: Search in Pinecone
    # Replace with your Pinecone index name
  search_results =  pinecone_index.query(
      vector=query_embedding.tolist(),
      top_k=2,  # Retrieve top 2 most relevant results
      include_metadata=True
  )

  # Step 3: Extract context from search results
  # Step 3: Extract context from search results
  # Instead of 'page_content', use 'text' which you used during upsert
  contexts = [result['metadata']['text'] for result in search_results['matches']]

  # Combine contexts for LLM
  context = "\n".join(contexts)

  # Step 4: Prepare prompt for Together.ai
  prompt = f"""
  You are a helpful assistant. Use the following retrieved documents to answer the question:
  Context:
  {context}
  Question: {question}
  Answer:
  """
  #llm=ChatmodelInstantiate(chat_model)
  llm = ChatTogether(api_key="c51c9bcaa6bf7fae3ce684206311564828c13fa2e91553f915fee01d517ccee9",
                  model=chat_model,  )


  # Step 5: Use Together.ai LLM for generation
  response = llm.predict(prompt)
  print(response)

def inference_weaviate(chat_model, question):
    from langchain_together import ChatTogether
    chat_model = ChatTogether(
        together_api_key="c51c9bcaa6bf7fae3ce684206311564828c13fa2e91553f915fee01d517ccee9",
        model=chat_model,
    )
    from langchain.prompts import ChatPromptTemplate
    template= """You are an expert financial advisor. Use the context to answer questions accurately and concisely:
    Context:
    {context}
    Question: {question}
    Answer(be specific and avoid hallucinations)::
    """
    prompt=ChatPromptTemplate.from_template(template)
    from langchain.schema.runnable import RunnablePassthrough
    from langchain.schema.output_parser import StrOutputParser
    output_parser=StrOutputParser()
    retriever=vs.as_retriever()
    rag_chain=(
    {"context":retriever,"question":RunnablePassthrough()}
      |prompt
      |chat_model
      |output_parser
    )
    result = rag_chain.invoke(question)
    return result

"""## Inference function:
###calls the inference functions for each vector db based on vectordb name taken as input
"""

def inference(vectordb_name, chat_model, question):
    if vectordb_name == "Chroma":
        inference_chroma(chat_model, question)
    elif vectordb_name == "FAISS":
        inference_faiss(chat_model, question)
    elif vectordb_name == "Qdrant":
        inference_qdrant(chat_model, question)
    elif vectordb_name == "Pinecone":
        inference_pinecone(chat_model, question)
    elif vectordb_name == "Weaviate":
        inference_weaviate(chat_model, question)
    else:
        print("Invalid Choice")

"""##Call preprocess and inference funtions
#### (restart runtime for change in embedding_model if embedding dimensions would change)


"""
#!pip install langchain-weaviate
import streamlit as st

def main():
    st.title("Vector Database and Chat Model Inference")

    # User inputs
    doc_path = st.text_input("Enter the path to the document:")
    embedding_model = st.selectbox(
        "Choose an embedding model:",
        [
            "all-MiniLM-L6-v2",
            "paraphrase-MiniLM-L6-v2",
            "distilbert-base-nli-stsb-mean-tokens",
            "all-distilroberta-v1",
            "paraphrase-distilroberta-base-v1",
            "stsb-roberta-base",
            "msmarco-distilbert-base-tas-b",
        ]
    )
    chunk_size = st.number_input("Enter chunk size:", min_value=1, value=1000, step=1)
    vectordb_name = st.selectbox(
        "Choose a vector database:",
        ["Pinecone", "Chroma", "FAISS", "Qdrant", "Weaviate"]
    )
    chat_model = st.selectbox(
        "Choose a chat model:",
        [
            "Qwen/QwQ-32B-Preview",
            "meta-llama/Llama-3.3-70B-Instruct-Turbo",
            "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
            "scb10x/scb10x-llama3-typhoon-v1-5-8b-instruct",
            "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        ]
    )
    question = st.text_input("Enter your question:")

    if st.button("Run Inference"):
        if doc_path and question:
            st.info("Processing... This may take a while.")

            # Preprocess and initialize vector DBs
            try:
                index, docstore, index_to_docstore_id, vector_store, retriever, client, pinecone_index, vs = preprocess_vectordbs(
                    doc_path, embedding_model, chunk_size
                )

                # Run inference
                answer = inference(vectordb_name, chat_model, question)
                st.success("Inference Complete!")
                st.write("Answer:", answer)
            except Exception as e:
                st.error(f"An error occurred: {e}")
        else:
            st.warning("Please provide all inputs before running inference.")

if __name__ == "__main__":
    main()



