# -*- coding: utf-8 -*-
"""Freezed code of Final PineconeQdrantFAISSandChroma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0xv2v3tc6MFMlQ2sxv1cZkGpI3gxEJ2

### Install dependencies
"""
# from dependencies import *
from preprocess_text import *
from preprocess import *
from preprocess_vectordbs import *
from inference import * 

def inference(vectordb_name, chat_model, question):
    if vectordb_name == "Chroma":
        inference_chroma(chat_model, question)
    elif vectordb_name == "FAISS":
        inference_faiss(chat_model, question)
    elif vectordb_name == "Qdrant":
        inference_qdrant(chat_model, question)
    elif vectordb_name == "Pinecone":
        inference_pinecone(chat_model, question)
    elif vectordb_name == "Weaviate":
        inference_weaviate(chat_model, question)
    else:
        print("Invalid Choice")

import streamlit as st

def main():
    st.title("Vector Database and Chat Model Inference")

    # User inputs
    doc_path = st.text_input("Enter the path to the document:")
    embedding_model = st.selectbox(
        "Choose an embedding model:",
        [
            "all-MiniLM-L6-v2",
            "paraphrase-MiniLM-L6-v2",
            "distilbert-base-nli-stsb-mean-tokens",
            "all-distilroberta-v1",
            "paraphrase-distilroberta-base-v1",
            "stsb-roberta-base",
            "msmarco-distilbert-base-tas-b",
        ]
    )
    chunk_size = st.number_input("Enter chunk size:", min_value=1, value=1000, step=1)
    vectordb_name = st.selectbox(
        "Choose a vector database:",
        ["Pinecone", "Chroma", "FAISS", "Qdrant", "Weaviate"]
    )
    chat_model = st.selectbox(
        "Choose a chat model:",
        [
            "Qwen/QwQ-32B-Preview",
            "meta-llama/Llama-3.3-70B-Instruct-Turbo",
            "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
            "scb10x/scb10x-llama3-typhoon-v1-5-8b-instruct",
            "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        ]
    )
    question = st.text_input("Enter your question:")

    if st.button("Run Inference"):
        if doc_path and question:
            st.info("Processing... This may take a while.")

            # Preprocess and initialize vector DBs
            try:
                index, docstore, index_to_docstore_id, vector_store, retriever, client, pinecone_index, vs = preprocess_vectordbs(
                    doc_path, embedding_model, chunk_size
                )

                # Run inference
                answer = inference(vectordb_name, chat_model, question)
                st.success("Inference Complete!")
                st.write("Answer:", answer)
            except Exception as e:
                st.error(f"An error occurred: {e}")
        else:
            st.warning("Please provide all inputs before running inference.")

if __name__ == "__main__":
    main()



